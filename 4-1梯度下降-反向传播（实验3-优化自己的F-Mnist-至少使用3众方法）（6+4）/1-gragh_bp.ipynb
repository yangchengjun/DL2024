{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.eye(4, 3) # 生成一个4x3的单位矩阵\n",
    "b = torch.full_like(a, 1)   # 生成一个与a形状相同的全1矩阵\n",
    "x = torch.add(a, 1, b)  # x = a + b,1是可选的缩放因子\n",
    "w = torch.randn(len(x[0]), 1, requires_grad=True)   # 生成一个随机数列，长度为x的列数，1是输出的列数\n",
    "# ---------------------------------上方是用户创建，下方是计算得到-------------------------------------------------\n",
    "y = torch.mm(x, w)  # 矩阵相乘\n",
    "z = 0.5*y**2    # 矩阵平方\n",
    "m = (z - 1)/2   # 矩阵减1再除2\n",
    "n = m - 0.1 # 矩阵减0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户创建的是叶子节点\n",
    "叶子节点在经过反向传播之后梯度值能够得以保留，非叶子节点的梯度值则为：None，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf:True\n",
      "b.is_leaf:True\n",
      "x.is_leaf:True\n",
      "w.is_leaf:True\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf:{a.is_leaf}')\n",
    "print(f'b.is_leaf:{b.is_leaf}')\n",
    "print(f'x.is_leaf:{x.is_leaf}')\n",
    "print(f'w.is_leaf:{w.is_leaf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.is_leaf:False\n",
      "z.is_leaf:False\n",
      "m.is_leaf:False\n",
      "n.is_leaf:False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'y.is_leaf:{y.is_leaf}')\n",
    "print(f'z.is_leaf:{z.is_leaf}')\n",
    "print(f'm.is_leaf:{m.is_leaf}')\n",
    "print(f'n.is_leaf:{n.is_leaf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)# 它由用户创建， requires_grad 默认False\n",
    "print(m.requires_grad)# 它计算得到， requires_grad 默认True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w: None \n",
      "grad_n: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1736/67028844.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print('grad_w:', w.grad, '\\ngrad_n:', n.grad)\n"
     ]
    }
   ],
   "source": [
    "# n.retain_grad()         # 保留n的梯度 不加报错： UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor\n",
    "print('grad_w:', w.grad, '\\ngrad_n:', n.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 叶子节点辅助运算，但是最后默认都释放了。所以在一次反向传播完成前叶子结点的值不能在原地址上被修改\n",
    "p-->x-->y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "tensor([[1.0806, 1.0806, 1.0806, 1.0806],\n",
      "        [1.0806, 1.0806, 1.0806, 1.0806],\n",
      "        [1.0806, 1.0806, 1.0806, 1.0806]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量，设置 requires_grad=True\n",
    "p = torch.ones(3, 4, requires_grad=True)\n",
    "# 对 p 进行操作\n",
    "x = torch.sin(p)\n",
    "y=x*2\n",
    "# 打印 p 的形状\n",
    "print(p.shape)\n",
    "# 由于 x 不是标量，需要传递一个与 x 形状相同的梯度\n",
    "gradient = torch.ones_like(x)\n",
    "x.retain_grad() #不保留会被释放\n",
    "# 执行反向传播，计算梯度\n",
    "y.backward(gradient)\n",
    "# 查看 p 的梯度\n",
    "print(x.grad)   # y=2x,dy/dx=2,所以x.grad=2\n",
    "print(p.grad)   # y=2sin(p),dy/dp=2cos(p),所以p.grad=2cos(p),cos1=0.5403\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2024",
   "language": "python",
   "name": "dl2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
